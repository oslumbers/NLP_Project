{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Critics \n",
    "This notebook generates positive and negative moview reviews from a finetuned GPT model.\n",
    "Code from huggingface has been used as a base and adapted to this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code in this module uses and extends the Huggingface chatbot Code \n",
    "#Original source for the chatbot code can be found at https://github.com/huggingface/transfer-learning-conv-ai\n",
    "#The full fine-tuning training run of the model needs to be executed from the bash shell once the conv-ai libraries have been installed\n",
    "#The python bash command to use is:\n",
    "#python ./train.py --gradient_accumulation_steps=2 --lm_coef=2.0 --max_history=1 --n_epochs=1 --num_candidates=2 --personality_permutations=1 --train_batch_size=2 --valid_batch_size=2 --dataset_path=\"./../notebooks/review_train.txt\" --dataset_cache=\"./../notebooks/review_train.bin\" --device=\"cuda\"\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "import socket\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from transformers import cached_path\n",
    "\n",
    "__file__ =\"log.txt\"\n",
    "logger = logging.getLogger(__file__)\n",
    "\n",
    "def download_pretrained_model():\n",
    "    \"\"\" Download and extract finetuned model from S3 \"\"\"\n",
    "    resolved_archive_file = cached_path(HF_FINETUNED_MODEL)\n",
    "    tempdir = tempfile.mkdtemp()\n",
    "    logger.info(\"extracting archive file {} to temp dir {}\".format(resolved_archive_file, tempdir))\n",
    "    with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
    "        archive.extractall(tempdir)\n",
    "    return tempdir\n",
    "\n",
    "\n",
    "def get_dataset(tokenizer, dataset_path, dataset_cache):\n",
    "    \"\"\" Get tokenized PERSONACHAT dataset from S3 or cache.\"\"\"\n",
    "    dataset_path = dataset_path or PERSONACHAT_URL\n",
    "    dataset_cache = dataset_cache + '_' + type(tokenizer).__name__  # To avoid using GPT cache for GPT-2 and vice-versa\n",
    "    if dataset_cache and os.path.isfile(dataset_cache):\n",
    "        logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
    "        dataset = torch.load(dataset_cache)\n",
    "    else:\n",
    "        logger.info(\"Download dataset from %s\", dataset_path)\n",
    "        personachat_file = cached_path(dataset_path)\n",
    "        with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            dataset = json.loads(f.read())\n",
    "\n",
    "        logger.info(\"Tokenize and encode the dataset\")\n",
    "        def tokenize(obj):\n",
    "            if isinstance(obj, str):\n",
    "                return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "            if isinstance(obj, dict):\n",
    "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "            return list(tokenize(o) for o in obj)\n",
    "        dataset = tokenize(dataset)\n",
    "        torch.save(dataset, dataset_cache)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "def make_logdir(model_name: str):\n",
    "    \"\"\"Create unique path to save results and checkpoints, e.g. runs/Sep22_19-45-59_gpu-7_gpt2\"\"\"\n",
    "    # Code copied from ignite repo\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    logdir = os.path.join(\n",
    "        'runs', current_time + '_' + socket.gethostname() + '_' + model_name)\n",
    "    return logdir\n",
    "\n",
    "#shuffle two lists in unison\n",
    "def unison_shuffles(a,b):\n",
    "    assert len(a)==len(b)\n",
    "    c = list(zip(a,b))\n",
    "    random.shuffle(c)\n",
    "    return zip(*c)\n",
    "\n",
    "#parse a line of text\n",
    "def parse_line(line):\n",
    "    line = line.strip().lower()\n",
    "    line = line.replace(\"&nbsp;\", \" \")\n",
    "    line = re.sub(r'<br(\\s\\/)?>', ' ', line)\n",
    "    line = re.sub(r' +', ' ', line)  # merge multiple spaces into one\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "from itertools import chain\n",
    "from pprint import pformat\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
    "\n",
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    if num_added_tokens > 0:\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "\n",
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance\n",
    "\n",
    "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(personality, history, tokenizer, model, args, current_output=None):\n",
    "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "    if current_output is None:\n",
    "        current_output = []\n",
    "\n",
    "    for i in range(args['max_length']):\n",
    "        instance = build_input_from_segments(personality, history, current_output, tokenizer, with_eos=False)\n",
    "\n",
    "        input_ids = torch.tensor(instance[\"input_ids\"], device=args['device']).unsqueeze(0)\n",
    "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=args['device']).unsqueeze(0)\n",
    "\n",
    "        logits=None\n",
    "        logits = model(input_ids, token_type_ids=token_type_ids)\n",
    "            \n",
    "        if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
    "            logits = logits[0]\n",
    "        logits = logits[0, -1, :] / args['temperature']\n",
    "        logits = top_filtering(logits, top_k=args['top_k'], top_p=args['top_p'])\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        prev = torch.topk(probs, 1)[1] if args['no_sample'] else torch.multinomial(probs, 1)\n",
    "        if i < args['min_length'] and prev.item() in special_tokens_ids:\n",
    "            while prev.item() in special_tokens_ids:\n",
    "                if probs.max().item() == 1:\n",
    "                    warnings.warn(\"Warning: model generating special token with probability 1.\")\n",
    "                    break  # avoid infinitely looping over special token\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if prev.item() in special_tokens_ids:\n",
    "            break\n",
    "        current_output.append(prev.item())\n",
    "\n",
    "    return current_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import string\n",
    "\n",
    "#We build a custom dataset of IMDB negative reviews.\n",
    "PATH='aclImdb/'\n",
    "names = ['neg','pos']\n",
    "\n",
    "#function to load the labels\n",
    "def load_texts_labels_from_folders(path, folders):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(folders):\n",
    "        for fname in glob(os.path.join(path, label, '*.*')):\n",
    "            texts.append(open(fname, 'r').read())\n",
    "            labels.append(idx)\n",
    "    # stored as np.int8 to save space \n",
    "    return texts, np.array(labels).astype(np.int8)\n",
    "\n",
    "negative_reviews_train = []\n",
    "positive_reviews_train = []\n",
    "negative_reviews_dev = []\n",
    "positive_reviews_dev = []\n",
    "\n",
    "X_train_raw,_ = load_texts_labels_from_folders(f'{PATH}train',names)\n",
    "X_dev_raw,_ = load_texts_labels_from_folders(f'{PATH}test',names)\n",
    "\n",
    "all_sets = {'train': {'positives':positive_reviews_train, 'negatives':negative_reviews_train, 'raw':X_train_raw}, \n",
    "           'dev': {'positives':positive_reviews_dev, 'negatives':negative_reviews_dev, 'raw':X_dev_raw}}\n",
    "\n",
    "for key, data_items in all_sets.items():\n",
    "    # read in the train data\n",
    "    for i in range(len(data_items['raw'])):\n",
    "        review = parse_line(data_items['raw'][i])\n",
    "        s =  [e.strip()+' .' for e in review.split('.') if e]\n",
    "        #further split the sentences up so that they are at most 170 characters or 60 tokens\n",
    "        #deal with max characters first\n",
    "        s_maxlen =[]\n",
    "        for si in s:\n",
    "            while len(si) >170:\n",
    "                s_170,si = si[0:170],si[170:]\n",
    "                s_maxlen.append(s_170)\n",
    "            s_maxlen.append(si)\n",
    "        #now with max tokens\n",
    "        s_maxtkn =[]\n",
    "        for si in s_maxlen:\n",
    "            si = si.split(' ')\n",
    "            while len(si) >60:\n",
    "                s_60,si = si[0:60],si[60:]\n",
    "                s_maxtkn.append(' '.join(s_60))\n",
    "            s_maxtkn.append(' '.join(si))\n",
    "\n",
    "        if(i>12499):        \n",
    "            data_items['positives'].append(s_maxtkn)\n",
    "        else:\n",
    "            data_items['negatives'].append(s_maxtkn)\n",
    "\n",
    "#now we shuffle\n",
    "random.shuffle(positive_reviews_train)\n",
    "random.shuffle(negative_reviews_train)\n",
    "random.shuffle(positive_reviews_dev)\n",
    "random.shuffle(negative_reviews_dev)\n",
    "\n",
    "#X_train,y_train=unison_shuffles(X_train,y_train)\n",
    "#X_dev,y_dev=unison_shuffles(X_dev,y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#we need to create the data to finetune the model\n",
    "\n",
    "train_entries = []\n",
    "\n",
    "#we define the movie critic personalities\n",
    "#positive and negative\n",
    "personalities = {\n",
    "    'Positive': {\n",
    "        'train_data': positive_reviews_train,\n",
    "        'personality': [\n",
    "            \"I'm an optimist .\",\n",
    "            \"I'm rarely disappointed by a film .\",\n",
    "            \"I am a film critic .\",\n",
    "            \"I love watching movies .\"\n",
    "        ],\n",
    "        'distractions': negative_reviews_train\n",
    "    },\n",
    "    'Negative': {\n",
    "        'train_data': negative_reviews_train,\n",
    "        'personality': [\n",
    "            \"I'm very critical .\",\n",
    "            \"I find bad acting disappointing .\",\n",
    "            \"I am a film critic .\",\n",
    "            \"I watch a lot of bad movies.\"\n",
    "        ],\n",
    "        'distractions': positive_reviews_train\n",
    "    }\n",
    "}\n",
    "\n",
    "#generate the input data in the correct format\n",
    "for key, traits in personalities.items():\n",
    "    #add the personality reviews\n",
    "    for review in traits['train_data']:\n",
    "        #add the personality type\n",
    "        entry = {\n",
    "            \"personality\": traits['personality'],\n",
    "            \"utterances\" : []\n",
    "        }\n",
    "        history= []\n",
    "        h=0\n",
    "        for sentence in review:\n",
    "            if h%2==0:\n",
    "                history.append(sentence)\n",
    "            else:\n",
    "                #we process this entry\n",
    "                #create 19 distractor candidates from the sentences of the opposing sentiment\n",
    "                candidates=[]\n",
    "                for i in range(19):\n",
    "                    candidates.append(random.choice(random.choice(traits['distractions'])))\n",
    "                #now append the gold truth\n",
    "                candidates.append(sentence)\n",
    "                #now add this\n",
    "                utterance = {\"candidates\": candidates, \"history\": history.copy()}\n",
    "                entry[\"utterances\"].append(utterance)\n",
    "                #append the current sentence to the history\n",
    "                history.append(sentence)\n",
    "            h+=1\n",
    "        #add this to the list\n",
    "        train_entries.append(entry)\n",
    "\n",
    "#now mix it all up\n",
    "random.shuffle(train_entries)\n",
    "\n",
    "format_entries = {'train': train_entries[0:10000],'valid': train_entries[10000:11000]}\n",
    "\n",
    "###Commented out - text to save the training results\n",
    "#with open('review_train.txt', 'w') as outfile:\n",
    "#    json.dump(format_entries, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out an example of the training data we are using \n",
    "for key,value in format_entries.items():\n",
    "    for listitem in value:        \n",
    "        for key2, value2 in listitem.items():\n",
    "            for l2 in range(len(value2)):\n",
    "                if(type(value2[l2]) is dict):\n",
    "                    for key3, value3 in value2[l2].items():\n",
    "                        print(\"\")\n",
    "                        print(f\"{key}->{key2}->{key3}\")\n",
    "                        for l3 in range(len(value3)):\n",
    "                            print(f\"({l3+1}) {value3[l3]}\")\n",
    "                else:\n",
    "                    print(\"\")\n",
    "                    print(f\"{key}->{key2}\")\n",
    "                    print(f\"({l2+1}) {value2[l2]}\")\n",
    "                #for key3, value3 in listitem2.items():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to generate reviews\n",
    "\n",
    "#use a random seed or -1 \n",
    "RNDSEED=42\n",
    "SAVED_MODEL='./../transfer-learning-conv-ai/runs/Mar08_13-02-33_ip-172-31-5-229_openai-gpt/'\n",
    "CACHED_PERSONALITY_TOKENS='./review_train.bin'\n",
    "PERSONALITY_DATA='./review_train.txt'\n",
    "\n",
    "if RNDSEED>-1:\n",
    "    random.seed(RNDSEED)\n",
    "    torch.random.manual_seed(RNDSEED)\n",
    "    torch.cuda.manual_seed(RNDSEED)\n",
    "\n",
    "#setup the model\n",
    "tokenizer_class, model_class = (OpenAIGPTTokenizer, OpenAIGPTLMHeadModel)\n",
    "tokenizer = tokenizer_class.from_pretrained(SAVED_MODEL)\n",
    "model = model_class.from_pretrained(SAVED_MODEL)\n",
    "model.to(\"cuda\")\n",
    "add_special_tokens_(model, tokenizer)\n",
    "\n",
    "#load the critic personalities\n",
    "dataset = get_dataset(tokenizer, PERSONALITY_DATA, CACHED_PERSONALITY_TOKENS)\n",
    "\n",
    "#positive critic encoding\n",
    "print(format_entries['train'][0]['personality'])\n",
    "print(dataset['train'][0]['personality'])\n",
    "positive_critic = dataset['train'][0]['personality']\n",
    "\n",
    "#negative critic encoding\n",
    "print(format_entries['train'][2]['personality'])\n",
    "print(dataset['train'][2]['personality'])\n",
    "negative_critic = dataset['train'][2]['personality']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a list of films to seed the model when generating random positive and negative reviews\n",
    "import pandas as pd\n",
    "df_movies = pd.read_csv('movies_metadata.csv')\n",
    "df_movies=df_movies.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty files\n",
    "if True==False:\n",
    "    with open('generated_positive_reviews.csv', 'w') as myfile:\n",
    "        myfile.write('')\n",
    "    \n",
    "    with open('generated_negative_reviews.csv', 'w') as myfile:\n",
    "        myfile.write('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate text\n",
    "import csv\n",
    "args={\n",
    "    'min_length': 50,\n",
    "    'max_length': 150,\n",
    "    'top_k': 0,\n",
    "    'temperature': 0.7,\n",
    "    'top_p' :0.95,\n",
    "    'device': 'cuda',\n",
    "    'no_sample':False,\n",
    "    'max_history': 5\n",
    "}\n",
    "\n",
    "#generate reviews.\n",
    "positive_reviews=[]\n",
    "negative_reviews=[]\n",
    "#reload the generated output\n",
    "with open('generated_positive_reviews.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    positive_reviews = list(reader)\n",
    "\n",
    "with open('generated_negative_reviews.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    negative_reviews = list(reader)\n",
    "\n",
    "personalities =[{'critic': positive_critic, 'save' :positive_reviews, 'file': 'generated_positive_reviews.csv'}\n",
    "                ,{'critic':negative_critic, 'save': negative_reviews, 'file' : 'generated_negative_reviews.csv'}]\n",
    "\n",
    "###Comment this this row out to run the generation (slow)\n",
    "#personalities =[]\n",
    "\n",
    "for personality in personalities:\n",
    "    while(len(personality['save'])<15000):\n",
    "        #create 1 review for each movie\n",
    "        for j in range(1):\n",
    "            history=[]\n",
    "            tokcnt=0\n",
    "            row_id=len(personality['save'])\n",
    "            seed_text= 'the movie was called ' + df_movies['title'][row_id] +'.'\n",
    "            review=seed_text\n",
    "            history.append(tokenizer.encode(seed_text))\n",
    "            tokcnt+=len(history[-1])\n",
    "            for i in range(10):\n",
    "                if(tokcnt<400):\n",
    "                    #print(review)\n",
    "                    with torch.no_grad():\n",
    "                        out_ids = sample_sequence(personality['critic'], history, tokenizer, model, args)\n",
    "                    history.append(out_ids)\n",
    "                    tokcnt+=len(history[-1])\n",
    "                    history = history[-(2*args['max_history']+1):]\n",
    "                    out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "                    review+=' '+ out_text\n",
    "            if(row_id%100==0):\n",
    "                print(row_id)\n",
    "                print(\"\\n\" +review+\"\\n\")\n",
    "            personality['save'].append(review)\n",
    "            with open(personality['file'], 'a') as myfile:\n",
    "                myfile.write(review+'\\n')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(personalities[0]['save'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload the generated output\n",
    "with open('generated_positive_reviews.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    positive_reviews = list(reader)[0]\n",
    "\n",
    "with open('generated_negative_reviews.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    negative_reviews = list(reader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print a couple of random examples\n",
    "print('A positive example, \\n')\n",
    "print(random.choice(positive_reviews))\n",
    "\n",
    "print('\\nA negative example, \\n')\n",
    "print(random.choice(negative_reviews))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
