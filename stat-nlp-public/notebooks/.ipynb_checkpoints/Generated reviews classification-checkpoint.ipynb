{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from transformers import (BertTokenizer, \n",
    "                          BertForSequenceClassification,\n",
    "                          DistilBertForSequenceClassification, \n",
    "                          DistilBertTokenizer)\n",
    "\n",
    "# Our code imports\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "import train_eval\n",
    "import synonym\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU {}!\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU :( using CPU\")\n",
    "\n",
    "importlib.reload(synonym)\n",
    "importlib.reload(train_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = np.genfromtxt('generated_negative_reviews.csv', delimiter='\\n', dtype=str)\n",
    "pos = np.genfromtxt('generated_positive_reviews.csv', delimiter='\\n', dtype=str)[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = np.concatenate([neg, pos])\n",
    "test_labels = np.concatenate([np.zeros(len(neg)), np.ones(len(pos))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "\n",
    "\n",
    "# Set up test data\n",
    "evaluation_data, _ = train_eval.ReviewDataset.setUpData(test_sentences, \n",
    "                                                           test_labels, \n",
    "                                                           tokenizer, 256,\n",
    "                                                       split = \"no_shuffle\")\n",
    "model = torch.load('yelp_bert.model')\n",
    "\n",
    "# evaluate\n",
    "acc, pred_labels, _, _, true_labels = train_eval.evaluate(model, evaluation_data, 16, return_pred_labels=True)\n",
    "\n",
    "pred = np.array([q for p in pred_labels for q in p]) \n",
    "true = np.array([q for p in true_labels for q in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'distilbert-base-cased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "\n",
    "\n",
    "# Set up test data\n",
    "evaluation_data, _ = train_eval.ReviewDataset.setUpData(test_sentences, \n",
    "                                                           test_labels, \n",
    "                                                           tokenizer, 256,\n",
    "                                                       split = \"no_shuffle\")\n",
    "model = torch.load('yelp_distil.model')\n",
    "\n",
    "# evaluate\n",
    "acc2, pred_labels2, _, _, true_labels2 = train_eval.evaluate(model, evaluation_data, 16, return_pred_labels=True)\n",
    "\n",
    "pred2 = np.array([q for p in pred_labels2 for q in p]) \n",
    "true2 = np.array([q for p in true_labels2 for q in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\"bert_accuracies\": acc,\n",
    "          \"bert_pred_labels\": pred,\n",
    "          \"bert_true_labels\": true,\n",
    "          \"distil_accuracies\": acc2,\n",
    "          \"distil_pred_labels\": pred2,\n",
    "          \"distil_true_labels\": true2}\n",
    "\n",
    "pickle.dump(output, open(\"gen_accs_preds.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(output[\"bert_pred_labels\"] == output[\"bert_true_labels\"]) / 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(output[\"bert_accuracies\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
